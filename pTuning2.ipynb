##20241120 
# 
# 

import os
import json
import torch
from transformers import (
    AutoProcessor,
    EncodecModel,
    TrainingArguments,
    Trainer,
    MusicgenForConditionalGeneration,
    T5EncoderModel #load t5 encoder only
)
from peft import PromptEncoderConfig, get_peft_model, TaskType
from datasets import Dataset
import librosa

# warnings.filterwarnings("ignore", category=FutureWarning)
# os.environ["TOKENIZERS_PARALLELISM"] = "false"

processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
T5EncoderModel._keys_to_ignore_on_load_unexpected = ["decoder.*"] #decoder무시하기 
model =T5EncoderModel.from_pretrained("t5-base") #t5-base
#model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

# T5
text_encoder = model

peft_config = PromptEncoderConfig(
    peft_type="P_TUNING",
    task_type=TaskType.SEQ_2_SEQ_LM,
    num_virtual_tokens=20,
    token_dim=text_encoder.config.hidden_size,
    num_transformer_submodules=1,
    num_attention_heads=text_encoder.config.num_heads,
    num_layers=text_encoder.config.num_layers,
    encoder_reparameterization_type="MLP",
    encoder_hidden_size=text_encoder.config.d_ff,
)

text_encoder = get_peft_model(text_encoder, peft_config)



#model.text_encoder = text_encoder

# for param in model.parameters():
#     param.requires_grad = False

# for param in model.text_encoder.parameters():
#     param.requires_grad = False

# for param in model.text_encoder.prompt_encoder.parameters():
#     param.requires_grad = True

#parameter_size = model.text_encoder.print_trainable_parameters()
parameter_size = text_encoder.print_trainable_parameters()

audio_encoder = EncodecModel.from_pretrained("facebook/encodec_32khz").eval()

data_dir = './dataset_wav'

data = {'text': [], 'audio': []}


for file_name in os.listdir(data_dir):
    if file_name.endswith('.json'):
        json_path = os.path.join(data_dir, file_name)
        with open(json_path, 'r') as f:
            json_data = json.load(f)
        description = json_data['description']
        base_name = os.path.splitext(file_name)[0]
        mp3_file = base_name + '.wav'
        mp3_path = os.path.join(data_dir, mp3_file)
        if os.path.exists(mp3_path):
            data['text'].append(description)
            data['audio'].append(mp3_path)
            
        else:
            print(f"Warning: mp3 file for {file_name} not found.")

dataset = Dataset.from_dict(data)

max_audio_length = 320000  # 10초 * 32,000Hz

def preprocess_function(example):
    audio_file = example['audio']
    try:
        audio_array, sr = librosa.load(audio_file, sr=32000)
    except Exception as e:
        print(f"Error loading {audio_file}: {e}")
        return None

    audio_tensor = torch.tensor(audio_array).unsqueeze(0).unsqueeze(1)  # Shape: (1, 1, sequence_length)

    if audio_tensor.shape[-1] > max_audio_length:
        audio_tensor = audio_tensor[..., :max_audio_length]
    elif audio_tensor.shape[-1] < max_audio_length:
        pad_size = max_audio_length - audio_tensor.shape[-1]
        audio_tensor = torch.nn.functional.pad(audio_tensor, (0, pad_size))

    inputs = processor(
        text=[example['text']],
        padding=True,
        return_tensors="pt",
    )

    with torch.no_grad():
        encoded_audio = audio_encoder.encode(audio_tensor)

    audio_codes = encoded_audio.audio_codes  # shape: (frames, batch_size, num_codebooks, seq_len)
    frames, batch_size, num_codebooks, seq_len = audio_codes.shape
    labels = audio_codes.reshape(batch_size * num_codebooks, seq_len)

    return {
        'input_ids': inputs['input_ids'][0],
        'attention_mask': inputs['attention_mask'][0],
        'labels': labels
    }


processed_dataset = dataset.map(preprocess_function, batched=False)
processed_dataset = processed_dataset.filter(lambda x: x is not None)

processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])


for processed_data in processed_dataset:
    print(processed_data['labels'].size())  # 또는 .shape
# for processed_data in processed_dataset:
#     print(processed_data.keys())  # processed_data의 키들을 출력하여 'labels'가 포함되어 있는지 확인


def data_collator(features):
    if len(features) == 0:
        print("no features")
    else:
    # features 내용 확인
        print(f"features sample: {features[0]}")  # 첫 번째 항목 확인
        print(f"features contains labels: {['labels' in f for f in features]}")  # 'labels' 키가 존재하는지 확인

    # 'labels'가 features에 포함되어 있는지 확인
    print(f"features contains labels: {[key for key in features[0].keys()]}")
        # 'labels'가 포함되었는지 확인
    for processed_data in processed_dataset:
        print(processed_data['labels'])  # 또는 .size()로 텐서 크기 확인

    return {
        'input_ids': torch.stack([f['input_ids'] for f in features]),
        'attention_mask': torch.stack([f['attention_mask'] for f in features]),
        'labels': torch.stack([f['labels'] for f in features]),
    }

# Training Arguments
training_args = TrainingArguments(
    output_dir='./musicgen_ptuning_results',
    per_device_train_batch_size=1,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=10,
    learning_rate=1e-4,
    remove_unused_columns=True,
)


import torch
from torch.utils.data import DataLoader
from transformers import AdamW, get_scheduler
from tqdm import tqdm

# 1. DataLoader 준비
train_dataloader = DataLoader(processed_dataset, batch_size=1, collate_fn=data_collator)
# 예시로 batch_size 1로 설정. 필요에 따라 변경

# 2. 모델, 옵티마이저, 스케줄러 준비
model.train()  # 모델을 학습 모드로 설정
optimizer = AdamW(model.parameters(), lr=1e-4)  # 옵티마이저 설정
num_epochs = 3  # 훈련할 에폭 수
num_training_steps = num_epochs * len(train_dataloader)  # 총 훈련 스텝 수
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,  # warmup 스텝 설정 (옵션)
    num_training_steps=num_training_steps,
)

# 3. 훈련 루프
for epoch in range(num_epochs):
    epoch_loss = 0.0  # 에폭 별 손실 값 초기화

    # tqdm을 사용하여 진행 상황 표시
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch + 1}/{num_epochs}"):

        # 배치에서 데이터 준비
        input_ids = batch['input_ids']  # 'device' 관련 코드 제거
        attention_mask = batch['attention_mask']
        labels = batch['labels']  # labels도 device 관련 코드 제거

        # 4. 모델 순전파
        optimizer.zero_grad()  # 이전 기울기 초기화
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss  # 손실 계산

        # 5. 손실 출력
        epoch_loss += loss.item()

        # 6. 역전파
        loss.backward()  # 기울기 계산
        optimizer.step()  # 가중치 업데이트
        lr_scheduler.step()  # 학습률 스케줄러 업데이트

    # 한 에폭이 끝날 때마다 평균 손실 출력
    print(f"Epoch {epoch + 1} Loss: {epoch_loss / len(train_dataloader)}")

    # 7. 모델 평가 (검증) - 필요한 경우
    # eval_loss = evaluate_model(validation_dataloader)
    # print(f"Validation Loss: {eval_loss}")
    
    # 8. 체크포인트 저장 (필요한 경우)
    # if (epoch + 1) % 10 == 0:
    #     model.save_pretrained(f'./model_checkpoint_epoch_{epoch + 1}')

# 모델 훈련 후 저장
model.save_pretrained('./final_model')



# # Trainer
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=processed_dataset,
#     data_collator=data_collator, 
#     remove_unused_columns=False, #Trainer 모델의 forward()는 메서드에 포함되지 않은 열을 제거하는데 , labels 소실 방지
#     # tokenizer=processor.tokenizer,  # tokenizer
# )
# trainer.train()
